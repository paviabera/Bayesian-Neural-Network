{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from pyro.optim import Adam\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tensors(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"Converts NumPy arrays into PyTorch tensors and performs train-test split.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if len(set(y)) > 1 else None\n",
    "    )\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)  # ðŸ”¹ Change to `torch.float32`\n",
    "    X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor  = torch.tensor(y_test, dtype=torch.float32)  # ðŸ”¹ Change to `torch.float32`\n",
    "\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    output_dim = 1  # ðŸ”¹ Should be 1 for binary classification with BCE loss\n",
    "\n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "def create_dummy_data():\n",
    "    # 10 datapoints, 2 features each\n",
    "    X = torch.tensor([\n",
    "        [0.1, 0.2],\n",
    "        [0.2, 0.1],\n",
    "        [0.4, 0.2],\n",
    "        [0.2, 0.3],\n",
    "        [0.9, 0.8],\n",
    "        [0.8, 0.9],\n",
    "        [0.3, 0.2],\n",
    "        [0.5, 0.6],\n",
    "        [0.6, 0.5],\n",
    "        [0.7, 0.8]\n",
    "    ], dtype=torch.float32)\n",
    "    # Binary labels: 1 if sum > 0.8, else 0.\n",
    "    y = (X.sum(dim=1) > 0.8).float()\n",
    "    input_dim, output_dim = X.shape[1], 1\n",
    "    return X, y, input_dim, output_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Cancer\n",
    "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
    "#     # Load the dataset\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     print(\"Loaded dataset with shape:\", df.shape)\n",
    "\n",
    "#     # Define target and feature columns correctly\n",
    "#     target_col = \"diagnosis\"\n",
    "#     feature_cols = df.columns[2:]  # Exclude 'id' (first column) and take features from column 3 onward\n",
    "\n",
    "#     # Extract features (X) and target labels (y)\n",
    "#     X = df[feature_cols].values\n",
    "#     y = df[target_col].values\n",
    "\n",
    "#     # Encode 'diagnosis' column ('M' -> 1, 'B' -> 0)\n",
    "#     le = LabelEncoder()\n",
    "#     y = le.fit_transform(y)  # Converts \"M\"/\"B\" to 1/0\n",
    "\n",
    "#     # Standardize features\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     # Check for NaNs and replace if necessary\n",
    "#     if np.isnan(X_scaled).any():\n",
    "#         print(\"âš ï¸ Warning: NaNs found in dataset! Replacing with zeros.\")\n",
    "#         X_scaled = np.nan_to_num(X_scaled)\n",
    "\n",
    "#     # Print class distribution\n",
    "#     unique, counts = np.unique(y, return_counts=True)\n",
    "#     print(\"Class Distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "#     # Ensure valid test size\n",
    "#     min_class_samples = min(counts)\n",
    "#     test_size = min(0.3, max(0.1, (min_class_samples - 1) / len(y)))\n",
    "\n",
    "#     # Perform train-test split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X_scaled, y, test_size=test_size, random_state=random_state, stratify=y if min_class_samples > 1 else None\n",
    "#     )\n",
    "#     # Convert to PyTorch tensors **before checking for NaNs**\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#     y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "#     X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "#     y_test_tensor  = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "#     # Now it's safe to check for NaNs\n",
    "#     print(\"NaNs in X_train:\", torch.isnan(X_train_tensor).sum().item())\n",
    "#     print(\"NaNs in y_train:\", torch.isnan(y_train_tensor).sum().item())\n",
    "\n",
    "\n",
    "#     input_dim = X_train_tensor.shape[1]\n",
    "#     output_dim = 1  # Binary classification, should be 1 for Pyro's Bernoulli\n",
    "\n",
    "#     return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # heart\n",
    "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     print(\"Loaded Heart dataset with shape:\", df.shape)\n",
    "\n",
    "#     # Define target and feature columns\n",
    "#     target_col = \"target\"\n",
    "#     feature_cols = df.columns[:-1]  # All columns except target\n",
    "\n",
    "#     # Extract features and target\n",
    "#     X = df[feature_cols].values\n",
    "#     y = df[target_col].values\n",
    "\n",
    "#     # Standardize numerical features\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(X)\n",
    "\n",
    "#     # Train-test split\n",
    "#     return prepare_tensors(X, y, test_size, random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabetis\n",
    "def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Loaded Diabetes dataset with shape:\", df.shape)\n",
    "\n",
    "    # Define target and feature columns\n",
    "    target_col = \"Outcome\"\n",
    "    feature_cols = df.columns[:-1]  # All except target\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[feature_cols].values)\n",
    "\n",
    "    y = df[target_col].values\n",
    "\n",
    "    # Train-test split\n",
    "    return prepare_tensors(X, y, test_size, random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # heart_statlog_cleveland_hungary_final\n",
    "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     print(\"Loaded Heart (Statlog + Cleveland + Hungary) dataset with shape:\", df.shape)\n",
    "\n",
    "#     # Define target and feature columns\n",
    "#     target_col = \"target\"\n",
    "#     feature_cols = df.columns[:-1]  # All except target\n",
    "\n",
    "#     # Convert multiclass labels (1, 2, 3, 4) â†’ Binary classification (1: Disease, 0: No Disease)\n",
    "#     df[target_col] = df[target_col].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "#     # Standardize features\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(df[feature_cols].values)\n",
    "\n",
    "#     y = df[target_col].values\n",
    "\n",
    "#     # Train-test split\n",
    "#     return prepare_tensors(X, y, test_size, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hepatitis\n",
    "\n",
    "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
    "#     df = pd.read_csv(file_path, na_values=\"?\")\n",
    "#     print(\"Loaded Hepatitis dataset with shape:\", df.shape)\n",
    "\n",
    "#     # Define target and feature columns\n",
    "#     target_col = \"out_class\"\n",
    "#     feature_cols = df.columns[1:]\n",
    "\n",
    "#     # Convert target labels (2 â†’ 1 \"Survived\", 1 â†’ 0 \"Died\")\n",
    "#     df[\"out_class\"] = df[\"out_class\"].replace({2: 1, 1: 0})\n",
    "\n",
    "#     # Convert binary categorical features (1 â†’ 0 \"No\", 2 â†’ 1 \"Yes\")\n",
    "#     binary_cols = [\"sex\", \"steroid\", \"antivirals\", \"fatigue\", \"malaise\", \"anorexia\",\n",
    "#                    \"liver_big\", \"liver_firm\", \"spleen_palable\", \"spiders\", \"ascites\",\n",
    "#                    \"varices\", \"histology\"]\n",
    "#     df[binary_cols] = df[binary_cols].replace({1: 0, 2: 1})\n",
    "\n",
    "#     # Fill missing values with column means\n",
    "#     df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "#     # Standardize numerical features\n",
    "#     numerical_cols = [\"age\", \"bilirubin\", \"alk_phosphate\", \"sgot\", \"albumin\", \"protime\"]\n",
    "#     scaler = StandardScaler()\n",
    "#     df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "#     # Extract features and target\n",
    "#     X = df[feature_cols].values\n",
    "#     y = df[\"out_class\"].values\n",
    "\n",
    "#     # Train-test split\n",
    "#     return prepare_tensors(X, y, test_size, random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingVINN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "\n",
    "    def model(X, y=None):\n",
    "        batch_size = X.shape[0]\n",
    "        input_dim = X.shape[1]      # dynamically get number of input features\n",
    "        hidden_dim = 3              # or set this as a parameter\n",
    "        # First layer: weights shape (input_dim, hidden_dim) and biases shape (hidden_dim)\n",
    "        w1 = pyro.sample(\"w1\", dist.Normal(torch.zeros(input_dim, hidden_dim), torch.ones(input_dim, hidden_dim)).to_event(2))\n",
    "        b1 = pyro.sample(\"b1\", dist.Normal(torch.zeros(hidden_dim), torch.ones(hidden_dim)).to_event(1))\n",
    "        # Second layer: weights shape (hidden_dim, 1) and bias scalar\n",
    "        w2 = pyro.sample(\"w2\", dist.Normal(torch.zeros(hidden_dim, 1), torch.ones(hidden_dim, 1)).to_event(2))\n",
    "        b2 = pyro.sample(\"b2\", dist.Normal(torch.tensor(0.0), torch.tensor(1.0)))\n",
    "        \n",
    "        hidden = torch.relu(torch.matmul(X, w1) + b1)\n",
    "        logits = torch.matmul(hidden, w2) + b2\n",
    "        pyro.deterministic(\"logits\", logits.squeeze(-1))\n",
    "        probs = torch.sigmoid(logits).squeeze(-1)\n",
    "        \n",
    "        with pyro.plate(\"data\", batch_size):\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(probs), obs=y)\n",
    "\n",
    "\n",
    "    def boosted_guide(X, y=None, index=0):\n",
    "        input_dim = X.shape[1]\n",
    "        hidden_dim = 3  # or parameterize this as needed\n",
    "        \n",
    "        w1_loc = pyro.param(f\"w1_loc_{index}\", torch.zeros(input_dim, hidden_dim))\n",
    "        w1_scale = pyro.param(f\"w1_scale_{index}\", torch.ones(input_dim, hidden_dim), constraint=dist.constraints.positive)\n",
    "        b1_loc = pyro.param(f\"b1_loc_{index}\", torch.zeros(hidden_dim))\n",
    "        b1_scale = pyro.param(f\"b1_scale_{index}\", torch.ones(hidden_dim), constraint=dist.constraints.positive)\n",
    "        \n",
    "        w2_loc = pyro.param(f\"w2_loc_{index}\", torch.zeros(hidden_dim, 1))\n",
    "        w2_scale = pyro.param(f\"w2_scale_{index}\", torch.ones(hidden_dim, 1), constraint=dist.constraints.positive)\n",
    "        b2_loc = pyro.param(f\"b2_loc_{index}\", torch.tensor(0.0))\n",
    "        b2_scale = pyro.param(f\"b2_scale_{index}\", torch.tensor(1.0), constraint=dist.constraints.positive)\n",
    "        \n",
    "        pyro.sample(\"w1\", dist.Normal(w1_loc, w1_scale).to_event(2))\n",
    "        pyro.sample(\"b1\", dist.Normal(b1_loc, b1_scale).to_event(1))\n",
    "        pyro.sample(\"w2\", dist.Normal(w2_loc, w2_scale).to_event(2))\n",
    "        pyro.sample(\"b2\", dist.Normal(b2_loc, b2_scale))\n",
    "\n",
    "            \n",
    "\n",
    "        # ------------------------------\n",
    "    # 4. Define the Custom RELBO Objective\n",
    "    # ------------------------------\n",
    "    def relbo(model, guide, X, y=None, approximation=None):\n",
    "        # Compute the standard ELBO loss using the current guide component\n",
    "        traced_guide = poutine.trace(guide).get_trace(X, y)\n",
    "        elbo = Trace_ELBO(max_plate_nesting=1)\n",
    "        loss = elbo.differentiable_loss(model, guide, X, y)\n",
    "        \n",
    "        # Use the current approximation (mixture of previous components) to compute its log-probability.\n",
    "        # We block the latent names so that previous components are not updated.\n",
    "        replayed_approx = poutine.trace(\n",
    "            poutine.replay(\n",
    "                poutine.block(approximation, expose=[\"w1\", \"b1\", \"w2\", \"b2\"]),\n",
    "                traced_guide\n",
    "            )\n",
    "        ).get_trace(X, y)\n",
    "        \n",
    "        relbo_value = -loss - replayed_approx.log_prob_sum()\n",
    "        # SVI minimizes the loss so we return negative of relbo_value.\n",
    "        return -relbo_value\n",
    "\n",
    "    # ------------------------------\n",
    "    # 5. Define the Mixture Approximation\n",
    "    #    This function samples a component according to mixture weights and calls its guide.\n",
    "    # ------------------------------\n",
    "    def approximation(X, y, components, weights):\n",
    "        assignment = pyro.sample(\"assignment\", dist.Categorical(weights))\n",
    "        return components[assignment.item()](X, y)\n",
    "    \n",
    "    # ðŸ”¹ Fix: Implement `forward()` to avoid `NotImplementedError`\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 6. Boosted Training Loop\n",
    "# ------------------------------\n",
    "def train_boosted_model(X, y,input_dim, output_dim, n_iterations=2, n_steps=2000):\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Start with an initial component (index=0)\n",
    "    initial_component = partial(BoostingVINN.boosted_guide, index=0)\n",
    "    # Make y optional in the lambda wrapper (defaulting to None)\n",
    "    components = [lambda X, y=None: initial_component(X, y)]\n",
    "    weights = torch.tensor([1.0])\n",
    "    wrapped_approximation = partial(BoostingVINN.approximation, components=components, weights=weights)\n",
    "    \n",
    "    optimizer_params = {\"lr\": 0.01, \"betas\": (0.90, 0.999)}\n",
    "    \n",
    "    # For tracking weights over iterations\n",
    "    mixture_weights_history = [weights.clone()]\n",
    "    \n",
    "    for t in range(1, n_iterations + 1):\n",
    "        print(f\"\\nBoosting iteration {t}\")\n",
    "        new_component = partial(BoostingVINN.boosted_guide, index=t)\n",
    "        # Wrap to standardize the signature with an optional y.\n",
    "        wrapped_new_component = lambda X, y=None, comp=new_component: comp(X, y)\n",
    "        \n",
    "        optimizer = Adam(optimizer_params)\n",
    "        svi = SVI(BoostingVINN.model, wrapped_new_component, optimizer,\n",
    "                  loss=partial(BoostingVINN.relbo, approximation=wrapped_approximation))\n",
    "        losses = []\n",
    "        for step in range(n_steps):\n",
    "            loss = svi.step(X, y)\n",
    "            losses.append(loss)\n",
    "            if step % 500 == 0:\n",
    "                print(f\"Step {step}: Loss = {loss}\")\n",
    "        print(f\"Component {t} training complete. Final loss: {losses[-1]:.2f}\")\n",
    "        \n",
    "        # Append the new guide component to our mixture\n",
    "        components.append(wrapped_new_component)\n",
    "        # Update the mixture weights; here a simple rule is used.\n",
    "        new_weight = 2.0 / (t + 1)\n",
    "        if t == 2:\n",
    "            new_weight = 0.5  # special case for demonstration\n",
    "        weights = weights * (1 - new_weight)\n",
    "        weights = torch.cat((weights, torch.tensor([new_weight])))\n",
    "        wrapped_approximation = partial(BoostingVINN.approximation, components=components, weights=weights)\n",
    "        print(f\"Updated mixture weights: {weights.tolist()}\")\n",
    "        \n",
    "        mixture_weights_history.append(weights.clone())\n",
    "    return components, weights, mixture_weights_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nll(probs, y, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Calculate the Negative Log Likelihood (NLL) for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        probs (torch.Tensor): Predicted probabilities (values between 0 and 1).\n",
    "        y (torch.Tensor): True binary labels (0 or 1).\n",
    "        epsilon (float): Small constant to avoid log(0).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed NLL value.\n",
    "    \"\"\"\n",
    "    nll = - torch.mean(y * torch.log(probs + epsilon) + (1 - y) * torch.log(1 - probs + epsilon))\n",
    "    return nll\n",
    "\n",
    "def calculate_ece(probs, y, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate the Expected Calibration Error (ECE).\n",
    "    \n",
    "    Args:\n",
    "        probs (torch.Tensor): Predicted probabilities.\n",
    "        y (torch.Tensor): True binary labels.\n",
    "        n_bins (int): Number of bins to use for calibration.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed ECE.\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays for easier manipulation\n",
    "    probs_np = probs.detach().cpu().numpy()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    \n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    total_samples = len(probs_np)\n",
    "    \n",
    "    # For each bin, compute the weighted absolute difference between accuracy and confidence.\n",
    "    for i in range(n_bins):\n",
    "        bin_lower = bin_boundaries[i]\n",
    "        bin_upper = bin_boundaries[i + 1]\n",
    "        bin_mask = (probs_np >= bin_lower) & (probs_np < bin_upper)\n",
    "        bin_count = np.sum(bin_mask)\n",
    "        if bin_count > 0:\n",
    "            avg_confidence = np.mean(probs_np[bin_mask])\n",
    "            avg_accuracy = np.mean(y_np[bin_mask])\n",
    "            bin_weight = bin_count / total_samples\n",
    "            ece += np.abs(avg_confidence - avg_accuracy) * bin_weight\n",
    "    return ece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified evaluation function to include NLL and ECE computation.\n",
    "def evaluate_boosted_model(X, y, components, weights, num_samples=1000):\n",
    "    aggregated_preds = None\n",
    "    for i, comp in enumerate(components):\n",
    "        predictive = Predictive(BoostingVINN.model, guide=comp, num_samples=num_samples)\n",
    "        samples = predictive(X)\n",
    "        # Get the average predicted probability over the samples\n",
    "        pred_probs = torch.sigmoid(samples[\"logits\"]).mean(0)\n",
    "        if aggregated_preds is None:\n",
    "            aggregated_preds = weights[i] * pred_probs\n",
    "        else:\n",
    "            aggregated_preds += weights[i] * pred_probs\n",
    "\n",
    "    # Ensure aggregated_preds is one-dimensional\n",
    "    aggregated_preds = aggregated_preds.squeeze()\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    final_preds = (aggregated_preds > 0.5).float()\n",
    "    accuracy = (final_preds == y).float().mean().item()\n",
    "    print(\"Boosted Model Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    \n",
    "    # Calculate and print Negative Log Likelihood (NLL)\n",
    "    nll_value = calculate_nll(aggregated_preds, y)\n",
    "    print(\"Negative Log Likelihood (NLL): {:.4f}\".format(nll_value.item()))\n",
    "    \n",
    "    # Calculate and print Expected Calibration Error (ECE)\n",
    "    ece_value = calculate_ece(aggregated_preds, y, n_bins=10)\n",
    "    print(\"Expected Calibration Error (ECE): {:.4f}\".format(ece_value))\n",
    "    \n",
    "    print(\"Aggregated Predictions (Probabilities):\", aggregated_preds)\n",
    "    print(\"True Labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------\n",
    "# # 7. Evaluation Function for the Boosted Model\n",
    "# # ------------------------------\n",
    "# def evaluate_boosted_model(X, y, components, weights, num_samples=1000):\n",
    "#     # Aggregate predictions over the mixture components using their weights.\n",
    "#     aggregated_preds = None\n",
    "#     for i, comp in enumerate(components):\n",
    "#         predictive = Predictive(model, guide=comp, num_samples=num_samples)\n",
    "#         # Note: we call with y omitted (defaulting to None)\n",
    "#         samples = predictive(X)\n",
    "#         # Average the deterministic logits (passed through sigmoid) to get probabilities.\n",
    "#         pred_probs = torch.sigmoid(samples[\"logits\"]).mean(0)\n",
    "#         if aggregated_preds is None:\n",
    "#             aggregated_preds = weights[i] * pred_probs\n",
    "#         else:\n",
    "#             aggregated_preds += weights[i] * pred_probs\n",
    "#     final_preds = (aggregated_preds > 0.5).float()\n",
    "#     accuracy = (final_preds == y).float().mean().item()\n",
    "#     print(\"Boosted Model Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "#     print(\"Aggregated Predictions:\", final_preds)\n",
    "#     print(\"True Labels:\", y)\n",
    "# ------------------------------\n",
    "# 8. Plotting Function for Mixture Weights\n",
    "# ------------------------------\n",
    "def plot_mixture_weights(mixture_weights_history):\n",
    "    # Use the final mixture weights for a bar plot.\n",
    "    final_weights = mixture_weights_history[-1].detach().cpu().numpy()\n",
    "    components_idx = np.arange(len(final_weights))\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(components_idx, final_weights, color='skyblue')\n",
    "    plt.xlabel(\"Boosting Component Index\")\n",
    "    plt.ylabel(\"Mixture Weight\")\n",
    "    plt.title(\"Mixture Weights of Boosting VI Components\")\n",
    "    plt.xticks(components_idx)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Diabetes dataset with shape: (768, 9)\n",
      "\n",
      "Boosting iteration 1\n",
      "Step 0: Loss = 365.5985412597656\n",
      "Step 500: Loss = 337.7944641113281\n",
      "Step 1000: Loss = 291.2571716308594\n",
      "Step 1500: Loss = 272.9897766113281\n",
      "Component 1 training complete. Final loss: 234.49\n",
      "Updated mixture weights: [0.0, 1.0]\n",
      "\n",
      "Boosting iteration 2\n",
      "Step 0: Loss = 301.2551574707031\n",
      "Step 500: Loss = 2203.63427734375\n",
      "Step 1000: Loss = -1.41893171283388e+18\n",
      "Step 1500: Loss = -1.769473793374332e+20\n",
      "Component 2 training complete. Final loss: -81330822330032586752.00\n",
      "Updated mixture weights: [0.0, 0.5, 0.5]\n",
      "Training time for BVI NN: 60.5990 seconds\n",
      "Boosted Model Accuracy: 76.54%\n",
      "Negative Log Likelihood (NLL): 0.5055\n",
      "Expected Calibration Error (ECE): 0.1156\n",
      "Aggregated Predictions (Probabilities): tensor([0.6402, 0.3171, 0.4540, 0.2649, 0.4092, 0.6344, 0.5272, 0.1572, 0.5343,\n",
      "        0.1371, 0.1507, 0.6301, 0.2165, 0.4906, 0.5084, 0.5715, 0.4995, 0.2296,\n",
      "        0.4918, 0.6126, 0.1743, 0.5760, 0.2922, 0.2810, 0.2703, 0.2231, 0.5967,\n",
      "        0.6283, 0.5244, 0.4943, 0.6726, 0.5706, 0.1570, 0.6151, 0.5539, 0.6291,\n",
      "        0.6090, 0.6512, 0.2650, 0.6349, 0.5555, 0.6308, 0.5601, 0.2624, 0.6151,\n",
      "        0.6276, 0.2298, 0.1640, 0.4666, 0.5942, 0.4965, 0.1892, 0.5609, 0.5542,\n",
      "        0.3283, 0.6138, 0.4986, 0.1290, 0.4169, 0.1719, 0.5322, 0.5187, 0.5897,\n",
      "        0.4411, 0.1426, 0.3058, 0.5447, 0.5980, 0.5126, 0.1706, 0.4781, 0.4482,\n",
      "        0.2520, 0.2545, 0.4738, 0.4906, 0.4024, 0.3316, 0.1623, 0.2384, 0.6352,\n",
      "        0.1763, 0.6816, 0.6017, 0.5720, 0.5965, 0.3020, 0.4232, 0.2097, 0.4640,\n",
      "        0.1710, 0.1743, 0.1508, 0.3863, 0.4704, 0.1708, 0.4071, 0.1736, 0.1546,\n",
      "        0.1822, 0.2070, 0.2692, 0.5724, 0.5100, 0.1700, 0.5977, 0.2464, 0.6001,\n",
      "        0.4941, 0.5155, 0.6495, 0.5278, 0.1859, 0.2367, 0.1285, 0.6422, 0.5347,\n",
      "        0.5159, 0.1498, 0.4380, 0.6342, 0.3095, 0.3233, 0.4665, 0.2030, 0.1963,\n",
      "        0.2536, 0.4786, 0.2430, 0.4561, 0.1786, 0.1434, 0.1823, 0.1576, 0.6642,\n",
      "        0.6278, 0.4081, 0.2196, 0.2272, 0.2196, 0.4298, 0.5495, 0.4787, 0.3256,\n",
      "        0.6826, 0.6465, 0.5927, 0.6003, 0.2005, 0.1372, 0.5669, 0.5889, 0.5040,\n",
      "        0.5854, 0.1551, 0.2184, 0.1868, 0.6443, 0.5888, 0.5585, 0.5514, 0.1724,\n",
      "        0.3227, 0.2789, 0.5646, 0.5938, 0.5630, 0.5062, 0.1612, 0.5910, 0.5245,\n",
      "        0.3663, 0.6567, 0.5005, 0.4758, 0.6481, 0.5171, 0.6098, 0.1819, 0.3316,\n",
      "        0.3722, 0.6059, 0.6656, 0.3408, 0.3066, 0.2171, 0.4384, 0.1412, 0.4348,\n",
      "        0.6249, 0.4635, 0.4852, 0.1343, 0.2901, 0.3424, 0.5319, 0.1827, 0.1766,\n",
      "        0.2012, 0.4617, 0.2410, 0.1744, 0.3663, 0.3811, 0.5136, 0.2582, 0.5952,\n",
      "        0.5769, 0.2403, 0.4688, 0.6266, 0.6379, 0.1423, 0.6195, 0.1758, 0.5590,\n",
      "        0.2368, 0.5477, 0.2048, 0.3268, 0.1965, 0.6429, 0.2108, 0.6375, 0.5270,\n",
      "        0.5881, 0.1922, 0.3000, 0.2047, 0.4441, 0.1494, 0.5988, 0.6652, 0.5488,\n",
      "        0.5774, 0.6192, 0.2549, 0.6453, 0.1889, 0.3075, 0.4956, 0.4125, 0.5397,\n",
      "        0.4885, 0.4787, 0.3267, 0.6275, 0.2875, 0.4483, 0.6151, 0.6241, 0.6092,\n",
      "        0.2040, 0.1937, 0.1438, 0.2163, 0.1535, 0.4062, 0.6388, 0.4502, 0.4448,\n",
      "        0.1918, 0.6107, 0.5003, 0.6713, 0.2612, 0.1662, 0.3148, 0.4179, 0.3054,\n",
      "        0.1450, 0.5935, 0.3691, 0.1329, 0.6245, 0.2736, 0.1727, 0.1693, 0.4589,\n",
      "        0.6184, 0.1527, 0.3836, 0.3283, 0.2513, 0.1391, 0.2372, 0.1490, 0.5965,\n",
      "        0.1690, 0.1999, 0.1483, 0.5465, 0.6162, 0.2541, 0.1213, 0.2047, 0.1441,\n",
      "        0.1949, 0.2389, 0.6333, 0.4932, 0.3499, 0.3966, 0.6250, 0.5057, 0.4828,\n",
      "        0.1472, 0.1942, 0.5742, 0.1433, 0.1566, 0.1728, 0.3671, 0.4627, 0.2009,\n",
      "        0.1234, 0.6032, 0.3599, 0.2757, 0.3687, 0.6591, 0.3272, 0.3600, 0.1664,\n",
      "        0.6378, 0.4484, 0.2817, 0.3671, 0.2028, 0.1268, 0.5990, 0.1585, 0.3990,\n",
      "        0.6292, 0.2614, 0.5395, 0.4936, 0.5804, 0.5678, 0.2090, 0.2075, 0.1640,\n",
      "        0.4613, 0.4228, 0.1497, 0.5658, 0.4875, 0.6655, 0.6284, 0.5472, 0.1608,\n",
      "        0.3951, 0.3734, 0.2052, 0.1776, 0.2259, 0.1623, 0.6736, 0.2516, 0.2771,\n",
      "        0.2302, 0.6085, 0.2173, 0.2688, 0.2859, 0.1189, 0.3565, 0.4782, 0.3498,\n",
      "        0.5415, 0.5657, 0.6174, 0.2977, 0.4741, 0.1518, 0.5560, 0.2863, 0.2979,\n",
      "        0.6174, 0.2116, 0.3077, 0.6083, 0.5628, 0.1775, 0.6452, 0.3014, 0.5061,\n",
      "        0.6154, 0.6106, 0.6359, 0.5089, 0.1713, 0.6315, 0.6075, 0.2487, 0.6383,\n",
      "        0.6323, 0.2066, 0.4384, 0.5742, 0.1756, 0.2529, 0.1642, 0.5813, 0.2041,\n",
      "        0.1694, 0.6375, 0.4693, 0.3241, 0.4067, 0.5327, 0.1440, 0.5385, 0.4636,\n",
      "        0.2812, 0.1441, 0.5030, 0.4649, 0.2361, 0.2486, 0.4198, 0.3182, 0.4932,\n",
      "        0.6275, 0.2108, 0.3183, 0.1644, 0.2006, 0.6202, 0.6046, 0.1587, 0.6094,\n",
      "        0.1831, 0.3067, 0.4982, 0.5069, 0.4607, 0.2419, 0.2065, 0.6107, 0.5325,\n",
      "        0.1696, 0.3015, 0.1567, 0.4759, 0.1941, 0.5046, 0.5152, 0.3539, 0.4878,\n",
      "        0.4821, 0.6100, 0.2573, 0.3981, 0.1696, 0.5997, 0.2991, 0.4319, 0.3046,\n",
      "        0.2506, 0.6125, 0.6233, 0.5672, 0.6407, 0.5177, 0.5250, 0.2501, 0.3492,\n",
      "        0.1523, 0.5446, 0.6100, 0.1688, 0.2467, 0.5919, 0.2177, 0.6051, 0.5829,\n",
      "        0.3043, 0.1550, 0.5107, 0.1958, 0.1666, 0.6068, 0.4905, 0.1718, 0.5099,\n",
      "        0.5961, 0.4361, 0.3893, 0.2808, 0.6675, 0.5214, 0.4736, 0.2479, 0.5972,\n",
      "        0.1538, 0.6488, 0.3745, 0.5195, 0.2603, 0.5299, 0.1545, 0.3282, 0.1588,\n",
      "        0.3980, 0.2146, 0.3132, 0.2839, 0.2039, 0.6389, 0.6214, 0.3293, 0.5908,\n",
      "        0.1457, 0.1727, 0.2114, 0.3862, 0.5417, 0.2145, 0.2000, 0.3733, 0.2068,\n",
      "        0.2382, 0.2399, 0.1490, 0.2878, 0.2489, 0.1778, 0.2014, 0.4874, 0.6390,\n",
      "        0.5774, 0.4302, 0.1902, 0.2094, 0.6576, 0.5360])\n",
      "True Labels: tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Inference time for BVI NN: 2.5968 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 9. Main Function\n",
    "# ------------------------------\n",
    "def main():\n",
    "    # X, y, input_dim, output_dim = create_dummy_data()\n",
    "    # print(\"Data X:\\n\", X)\n",
    "    # print(\"Data y:\\n\", y)\n",
    "    file_path = \"DataSets/diabetes.csv\"\n",
    "    X, X_test_tensor, y, y_test_tensor, input_dim, output_dim = load_and_preprocess_data(file_path)\n",
    "    n_iterations = 2  # number of boosting iterations; increase as needed\n",
    "    start_train_vi = time.time()\n",
    "    components, weights, mixture_weights_history = train_boosted_model(X, y,input_dim, output_dim, n_iterations=n_iterations, n_steps=2000)\n",
    "    end_train_vi = time.time()\n",
    "\n",
    "    print(\"Training time for BVI NN: {:.4f} seconds\".format(end_train_vi - start_train_vi))\n",
    "\n",
    "    start_infer_vi = time.time()\n",
    "    evaluate_boosted_model(X, y, components, weights, num_samples=500)\n",
    "    end_infer_vi = time.time()\n",
    "    print(\"Inference time for BVI NN: {:.4f} seconds\".format(end_infer_vi - start_infer_vi))\n",
    "    # plot_mixture_weights(mixture_weights_history)\n",
    "\n",
    "\n",
    "\n",
    "    # Use the test set tensor for generating the predictive distribution graph\n",
    "    x_test_tensor_for_graph = X_test_tensor  # Already a tensor from load_and_preprocess_data\n",
    "    num_samples_graph = 1000  # Number of Monte Carlo samples for smoother estimates\n",
    "    ensemble_samples = []  # Will store one ensemble sample per MC iteration\n",
    "\n",
    "    for i in range(num_samples_graph):\n",
    "        # Initialize an empty tensor for the ensemble prediction\n",
    "        ensemble_pred = torch.zeros(x_test_tensor_for_graph.shape[0])\n",
    "        # For each component in the ensemble, generate one sample and weight it accordingly\n",
    "        for comp, weight in zip(components, weights):\n",
    "            # Use Predictive with num_samples=1 to get one sample from the current component\n",
    "            predictive = Predictive(BoostingVINN.model, guide=comp, num_samples=1)\n",
    "            # \"logits\" is registered as deterministic in the model so we can retrieve it\n",
    "            sample = predictive(x_test_tensor_for_graph)[\"logits\"].squeeze()  # shape: [batch_size]\n",
    "            prob_sample = torch.sigmoid(sample)  # Convert logits to probability\n",
    "            ensemble_pred += weight * prob_sample\n",
    "        ensemble_samples.append(ensemble_pred)\n",
    "    ensemble_samples = torch.stack(ensemble_samples)  # shape: [num_samples_graph, batch_size]\n",
    "\n",
    "    # Compute the mean and standard deviation for each test sample\n",
    "    pred_mean = ensemble_samples.mean(dim=0).detach().numpy()\n",
    "    pred_std = ensemble_samples.std(dim=0).detach().numpy()\n",
    "\n",
    "    # Plot the predictive distribution with enhanced aesthetics\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    x_axis = np.arange(len(pred_mean))\n",
    "    plt.errorbar(x_axis, pred_mean, yerr=pred_std, fmt='o', \n",
    "                 color='forestgreen', ecolor='gray', capsize=4, label='Boosting VI Predictive Distribution')\n",
    "    plt.fill_between(x_axis, pred_mean - pred_std, pred_mean + pred_std, \n",
    "                     color='forestgreen', alpha=0.2)\n",
    "    plt.xlabel('Test Sample Index', fontsize=14)\n",
    "    plt.ylabel('Predicted Probability', fontsize=14)\n",
    "    plt.title('Predictive Distribution on Diabetes Dataset (Boosting VI NN)', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. just run \n",
    "# 2. run for 16 hidden layers\n",
    "# 3. run for booting 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
